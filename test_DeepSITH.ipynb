{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make final prediciton results for Kaggle submission (Deep_Sith)\n",
    "Use  PyTorch 1.4.0 Py3.7 Kernal/ Container on Rivanna\n",
    "Try to use V100 GPU, since it is much faster than the others\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pip install the necessary packages and download the [SITH_Layer_master] folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --user mne\n",
    "#!pip install --user seaborn\n",
    "#### pytorch and Cuda should be set up correctly on the Pytorch kernal or pytorch container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.cuda.DoubleTensor'>\n"
     ]
    }
   ],
   "source": [
    "from src.eval import evaluation\n",
    "from src.train_util import *\n",
    "from models.Deep_isith_EEG_model import *\n",
    "from models.LSTM_EEG_model import *\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from src.train_util import *\n",
    "from models.Deep_isith_EEG_model import *\n",
    "from models.LSTM_EEG_model import *\n",
    "\n",
    "# read config file\n",
    "import configparser\n",
    "import argparse\n",
    "\n",
    "# preprocessing\n",
    "import mne\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "import torch.nn\n",
    "import torch.nn.functional as F\n",
    "ttype = torch.cuda.DoubleTensor if torch.cuda.is_available() else torch.DoubleTensor\n",
    "labeltype = torch.cuda.LongTensor if torch.cuda.is_available() else torch.LongTensor\n",
    "print(ttype)\n",
    "from torch.utils.data import Dataset,DataLoader \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# training \n",
    "from torch import nn as nn\n",
    "from math import factorial\n",
    "import random\n",
    "import seaborn as sn\n",
    "import os \n",
    "from os.path import join\n",
    "import glob\n",
    "\n",
    "# validation\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score, matthews_corrcoef,confusion_matrix,plot_roc_curve\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, test_loader):\n",
    "    \"\"\"\n",
    "    \n",
    "    Iterate through each batch and make prediciton and calculate performance metrics\n",
    "    Use **matthews correlation coeefficient** since the data are imbanlanced\n",
    "    Again \n",
    "    Signals need to be in correct format. validation input: [nbatch x 1 x nFeutures x time] tensor.\n",
    "\n",
    "    The target has dimension of [time] tensor, in which each entry should be one of the numbers in \n",
    "    {0,1,2, ... K} at any time point.  \n",
    "    \n",
    "    \"\"\"\n",
    "    inferenced_y = np.empty(0)\n",
    "    for _, (val_x) in enumerate(test_loader):\n",
    "        #print(val_x.shape)\n",
    "        # the actual tensor\n",
    "        out_val = model(val_x)\n",
    "        #print(out_val.shape)\n",
    "        # pass through a softmax to tansform to probability on the third dimention (nbatch, seq, outFeature)\n",
    "        res = torch.nn.functional.softmax(out_val, dim=2)\n",
    "        #print(res.shape)\n",
    "        # predict should also be the second dimension [1] to clauclate AUC\n",
    "        y_pred = res[:,:,1]\n",
    "\n",
    "        # flatten the predicted result \n",
    "        y_score = np.ndarray.flatten(y_pred.detach().cpu().numpy())\n",
    "        \n",
    "        inferenced_y = np.concatenate((inferenced_y, y_score), axis=0)\n",
    "\n",
    "    return inferenced_y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sith parameters. Have to be exactly the same with the training DeepSITH parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sith_params1 = {\"in_features\":32, \n",
    "                        \"tau_min\":1, \"tau_max\":50, \n",
    "                        \"k\":23, 'dt':1,\n",
    "                        \"ntau\":10, 'g':0.0,  \n",
    "                        \"ttype\":ttype, \n",
    "                        \"hidden_size\":20, \"act_func\":nn.ReLU()}\n",
    "\n",
    "sith_params2 = {\"in_features\":sith_params1['hidden_size'], \n",
    "                        \"tau_min\":1, \"tau_max\":200.0,  \n",
    "                        \"k\":12, 'dt':1,\n",
    "                        \"ntau\":10, 'g':0.0, \n",
    "                        \"ttype\":ttype, \n",
    "                        \"hidden_size\":20, \"act_func\":nn.ReLU()}\n",
    "sith_params3 = {\"in_features\":sith_params2['hidden_size'], \n",
    "                    \"tau_min\":1, \"tau_max\":800.0,  \n",
    "                    \"k\":7, 'dt':1,\n",
    "                    \"ntau\":10, 'g':0.0, \n",
    "                    \"ttype\":ttype, \n",
    "                    \"hidden_size\":20, \"act_func\":nn.ReLU()}\n",
    "layer_params = [sith_params1, sith_params2,sith_params3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep_SITH dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# override EEGDataset in src\n",
    "class EEGDataset_pred(Dataset):\n",
    "    \"\"\"\n",
    "    A pytorch dataset\n",
    "    input shapes:\n",
    "        train_x: [nbatch, channels, sequence]\n",
    "        train_y: [nbatch,  sequence]\n",
    "    \n",
    "    Output shape:\n",
    "        Need to add a magic second dimension in order for Deep_sith\n",
    "        to work properly\n",
    "        train_x: [nbatch, 1, channels, sequence]\n",
    "        train_y: [nbatch,  sequence]\n",
    "    \"\"\"\n",
    "    def __init__(self, test_x):\n",
    "        self.test_x = test_x\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.test_x.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "                \n",
    "        return (self.test_x[idx].unsqueeze(0))\n",
    "        \n",
    "def load_test_data(test_x_t ,batch_size = 1):\n",
    "    # batch_size is a hyper parameter to tune \n",
    "    dataset = EEGDataset_pred(test_x_t)\n",
    "\n",
    "    # get the entire length of the dataset\n",
    "    dataset_size = len(dataset)\n",
    "\n",
    "    test_loader = DataLoader(dataset= dataset, batch_size=batch_size, \n",
    "                             shuffle=False)\n",
    "\n",
    "    return (test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read config file from ./config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from src.train_util import *\n",
    "from models.Deep_isith_EEG_model import *\n",
    "from models.LSTM_EEG_model import *\n",
    "import pandas as pd\n",
    "# read config file\n",
    "import configparser\n",
    "import argparse\n",
    "\n",
    "# enable use of command line\n",
    "parser = argparse.ArgumentParser(description='Input config files')\n",
    "parser.add_argument('--config', default = 'config/testing_config_Deep_isith.ini', type=str,\n",
    "                    help='an integer for the accumulator')\n",
    "opt, _ = parser.parse_known_args()\n",
    "\n",
    "# parser to read parameters\n",
    "config = configparser.ConfigParser()\n",
    "config.sections()\n",
    "\n",
    "# parameters from config file\n",
    "results = []\n",
    "config.read(opt.config)\n",
    "dir = config['data']['directory']\n",
    "subject_num = int(config['data']['subject #'])\n",
    "kernel_size = int(config['training']['kernel_size'])# sliding window size to use\n",
    "step = int(config['training']['step']) #  --the step between each slice. means overlap between batches is 1- step \n",
    "modelName = config['training']['model']\n",
    "# num of epochs to train\n",
    "nepochs = int(config['training']['nepochs'])\n",
    "loss_func =  torch.nn.CrossEntropyLoss()\n",
    "batch_size = int(config['training']['batch_size']) # batch_size is a hyper parameter to tune \n",
    "train_split = float(config['training']['train_split'])\n",
    "lr = float(config['training']['lr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Override the function *filter_standardization* in src. Add zero paddings for training and predicitons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# currently override the function in src\n",
    "def filter_standardization(raw,window_size = 1000,\n",
    "                          l_freq = 0,h_freq = 30, verbose = False, read_event = True):\n",
    "    \"\"\"\n",
    "    raw: raw object from mnew\n",
    "    window_size: rolling window_size for standardization,\n",
    "    l_freq, h_freq: frequency filters\n",
    "    nClass: the number of event channel to use \n",
    "    \"\"\"\n",
    "\n",
    "    filtered_X = raw.filter(l_freq=l_freq, h_freq= h_freq, method='fir',phase=\"minimum\",\n",
    "                            verbose= verbose, picks = [x for x in range(32)])\n",
    "    filtered_X = filtered_X.to_data_frame().drop(['time'],axis=1)\n",
    "    #print(filtered_X)\n",
    "    # insert a padding dataframe to add to the beginning of the dataframe\n",
    "    # this is to help sliding window standardization\n",
    "    \n",
    "    colNames = filtered_X.columns\n",
    "    padding_df = pd.DataFrame(np.zeros((window_size-1, 32)), columns = colNames)\n",
    "    filtered_X = padding_df.append(filtered_X, ignore_index=True)\n",
    "    #print(filtered_X)\n",
    "    # only the first 32 channels to standardize\n",
    "    filtered_X = filtered_X.iloc[:,0:32]\n",
    "    filtered_standardized = ((filtered_X - filtered_X.rolling(window_size).mean()) / filtered_X.rolling(window_size).std()).dropna()\n",
    "    # filtered and stardardized training data\n",
    "    input_signal = filtered_standardized.to_numpy()\n",
    "    input_signal = np.swapaxes(input_signal,0,1)\n",
    "\n",
    "    data = raw.get_data()\n",
    "    \n",
    "    if read_event:\n",
    "        # strip the first 99 data points due to rolling window implementation\n",
    "        target_signal = target_signal_val =data[32:38,window_size-1:] # export all channels, use only one channel eventually\n",
    "        #print(input_signal.shape,target_signal.shape)\n",
    "    else:\n",
    "        target_signal = None\n",
    "    # reformatt into tensor\n",
    "#     input_tensor = ttype(input_signal.reshape(1,1,input_signal.shape[0],-1))\n",
    "#     target_tensor = labeltype(target_signal.reshape(-1))\n",
    "\n",
    "    #print(input_tensor.shape, target_tensor.shape)\n",
    "    return (input_signal, target_signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to load Subject1 Data.\n",
      "subj1_series9\n",
      "torch.Size([1, 32, 115953])\n",
      "torch.Size([1, 32, 115953])\n",
      "subj1_series10\n",
      "torch.Size([1, 32, 117128])\n",
      "torch.Size([1, 32, 117128])\n",
      "Starting to load Subject2 Data.\n",
      "subj2_series9\n",
      "torch.Size([1, 32, 147373])\n",
      "torch.Size([1, 32, 147373])\n",
      "subj2_series10\n",
      "torch.Size([1, 32, 150712])\n",
      "torch.Size([1, 32, 150712])\n",
      "Starting to load Subject3 Data.\n",
      "subj3_series9\n",
      "torch.Size([1, 32, 111945])\n",
      "torch.Size([1, 32, 111945])\n",
      "subj3_series10\n",
      "torch.Size([1, 32, 113394])\n",
      "torch.Size([1, 32, 113394])\n",
      "Starting to load Subject4 Data.\n",
      "subj4_series9\n",
      "torch.Size([1, 32, 121267])\n",
      "torch.Size([1, 32, 121267])\n",
      "subj4_series10\n",
      "torch.Size([1, 32, 123527])\n",
      "torch.Size([1, 32, 123527])\n",
      "Starting to load Subject5 Data.\n",
      "subj5_series9\n",
      "torch.Size([1, 32, 131823])\n",
      "torch.Size([1, 32, 131823])\n",
      "subj5_series10\n",
      "torch.Size([1, 32, 129237])\n",
      "torch.Size([1, 32, 129237])\n",
      "Starting to load Subject6 Data.\n",
      "subj6_series10\n",
      "torch.Size([1, 32, 142668])\n",
      "torch.Size([1, 32, 142668])\n",
      "subj6_series9\n",
      "torch.Size([1, 32, 136853])\n",
      "torch.Size([1, 32, 136853])\n",
      "Starting to load Subject7 Data.\n",
      "subj7_series10\n",
      "torch.Size([1, 32, 138957])\n",
      "torch.Size([1, 32, 138957])\n",
      "subj7_series9\n",
      "torch.Size([1, 32, 138067])\n",
      "torch.Size([1, 32, 138067])\n",
      "Starting to load Subject8 Data.\n",
      "subj8_series9\n",
      "torch.Size([1, 32, 123509])\n",
      "torch.Size([1, 32, 123509])\n",
      "subj8_series10\n",
      "torch.Size([1, 32, 126755])\n",
      "torch.Size([1, 32, 126755])\n",
      "Starting to load Subject9 Data.\n",
      "subj9_series9\n",
      "torch.Size([1, 32, 125715])\n",
      "torch.Size([1, 32, 125715])\n",
      "subj9_series10\n",
      "torch.Size([1, 32, 126685])\n",
      "torch.Size([1, 32, 126685])\n",
      "Starting to load Subject10 Data.\n",
      "subj10_series9\n",
      "torch.Size([1, 32, 128331])\n",
      "torch.Size([1, 32, 128331])\n",
      "subj10_series10\n",
      "torch.Size([1, 32, 128906])\n",
      "torch.Size([1, 32, 128906])\n",
      "Starting to load Subject11 Data.\n",
      "subj11_series9\n",
      "torch.Size([1, 32, 136519])\n",
      "torch.Size([1, 32, 136519])\n",
      "subj11_series10\n",
      "torch.Size([1, 32, 140978])\n",
      "torch.Size([1, 32, 140978])\n",
      "Starting to load Subject12 Data.\n",
      "subj12_series10\n",
      "torch.Size([1, 32, 142200])\n",
      "torch.Size([1, 32, 142200])\n",
      "subj12_series9\n",
      "torch.Size([1, 32, 145669])\n",
      "torch.Size([1, 32, 145669])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# make predictions\n",
    "colName = ['id','HandStart', 'FirstDigitTouch', 'BothStartLoadPhase',\n",
    "          'LiftOff', 'Replace', 'BothReleased']\n",
    "#final dataframe\n",
    "df = pd.DataFrame(columns = colName)\n",
    "for j in range(1,13): # out loop train all subjects\n",
    "    results = []\n",
    "    subject_num = j # ignore the config subject num\n",
    "    # load data and do preprocessing\n",
    "    test_x_list = []\n",
    "    #test_y_list = []\n",
    "    print(f\"Starting to load Subject{subject_num} Data.\")\n",
    "    # loop for each series\n",
    "    for file in os.listdir(dir):\n",
    "        sub_idx = file.find('_')\n",
    "        if file[:-4].endswith('_data') & (file[4:sub_idx] == str(subject_num)):\n",
    "            # get the name\n",
    "            sub_idx_all = [i for i in range(len(file)) if file.startswith('_', i)]\n",
    "            id_s = file[:sub_idx_all[1]]\n",
    "            print(file[:sub_idx_all[1]])\n",
    "            # No events file for the test datset\n",
    "            raw = creat_mne_raw_object(dir+file,read_events=False)\n",
    "            # filter all channels\n",
    "            input_signal,_ = filter_standardization(raw,window_size = 1000,\n",
    "                                l_freq = 0,h_freq = 30, read_event = False)\n",
    "\n",
    "            input_tensor = ttype(input_signal.reshape(1,1,input_signal.shape[0],-1))\n",
    "            #target_tensor = labeltype(target_signal.reshape(6,-1)) # should be six channels\n",
    "            input_tensor = input_tensor.squeeze(0)\n",
    "            print(input_tensor.shape)\n",
    "            ###########for testing do not patch data ###########\n",
    "            #patches_test = input_tensor.unfold(dimension = 1, \n",
    "                                                #size = kernel_size, \n",
    "                                                #step = step).permute(1,0,2)\n",
    "            #patches_label = target_tensor.unfold(1, kernel_size, step).permute(1,0,2)\n",
    "            \n",
    "            #print(patches_train.shape, patches_label.shape)\n",
    "\n",
    "            # append to a list\n",
    "            #test_x_list.append(patches_test)\n",
    "            #train_y_list.append(patches_label)  \n",
    "\n",
    "            # make prediction one by one\n",
    "            test_x_t = input_tensor\n",
    "            #test_y_t = torch.cat(train_y_list, dim=0)\n",
    "            print(test_x_t.shape)\n",
    "    \n",
    "            # make predictions\n",
    "            df_pred = pd.DataFrame(columns = colName)\n",
    "            # another loop for each event\n",
    "            for i in range(1,7): # There are six events 1 - 6\n",
    "                # make a copy of every dict don't want to change them\n",
    "                layer_params_l = [sith_params1.copy(), \n",
    "                                  sith_params2.copy(),sith_params3.copy()]\n",
    "                nClass = i - 1\n",
    "                # j is the current subject\n",
    "                file = f'Deep_isith_Subject{j}_numEvent{nClass}.pth'\n",
    "                PATH = 'saved_NNs/' + file\n",
    "                model1 = DeepSITH_Tracker(out=2,\n",
    "                                         layer_params=layer_params_l, dropout=0.1).double()\n",
    "                model1.load_state_dict(torch.load(PATH))\n",
    "                model1.eval()\n",
    "                device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "                model1.to(device)\n",
    "\n",
    "                #train_y_t_nClass = train_y_t[:,nClass,:]\n",
    "                # create dataloader class\n",
    "                #print(f'batch_size is {batch_size}')\n",
    "                test_loader = load_test_data(test_x_t,\n",
    "                                batch_size = batch_size)\n",
    "                y_pred = predict(model1, test_loader)\n",
    "                \n",
    "                # +1 beacuse we have id\n",
    "                df_pred[colName[nClass+1]] = y_pred\n",
    "\n",
    "                df_pred['id'] = [f'{id_s}_{data_num}' for data_num in range(0, len(df_pred))]\n",
    "\n",
    "                # end of loop\n",
    "            # append to the final df\n",
    "            df = df.append(df_pred,ignore_index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>HandStart</th>\n",
       "      <th>FirstDigitTouch</th>\n",
       "      <th>BothStartLoadPhase</th>\n",
       "      <th>LiftOff</th>\n",
       "      <th>Replace</th>\n",
       "      <th>BothReleased</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>subj1_series9_0</td>\n",
       "      <td>0.003512</td>\n",
       "      <td>0.004995</td>\n",
       "      <td>0.006381</td>\n",
       "      <td>0.005685</td>\n",
       "      <td>0.028141</td>\n",
       "      <td>0.016946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>subj1_series9_1</td>\n",
       "      <td>0.002071</td>\n",
       "      <td>0.012519</td>\n",
       "      <td>0.001428</td>\n",
       "      <td>0.001229</td>\n",
       "      <td>0.026249</td>\n",
       "      <td>0.014232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>subj1_series9_2</td>\n",
       "      <td>0.001206</td>\n",
       "      <td>0.016747</td>\n",
       "      <td>0.000523</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>0.011038</td>\n",
       "      <td>0.010686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>subj1_series9_3</td>\n",
       "      <td>0.000640</td>\n",
       "      <td>0.024042</td>\n",
       "      <td>0.000593</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.004355</td>\n",
       "      <td>0.022294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>subj1_series9_4</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>0.064571</td>\n",
       "      <td>0.000786</td>\n",
       "      <td>0.000189</td>\n",
       "      <td>0.002088</td>\n",
       "      <td>0.018881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3144166</th>\n",
       "      <td>subj12_series9_145664</td>\n",
       "      <td>0.035969</td>\n",
       "      <td>0.004423</td>\n",
       "      <td>0.006587</td>\n",
       "      <td>0.014184</td>\n",
       "      <td>0.005188</td>\n",
       "      <td>0.001910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3144167</th>\n",
       "      <td>subj12_series9_145665</td>\n",
       "      <td>0.034196</td>\n",
       "      <td>0.004681</td>\n",
       "      <td>0.006590</td>\n",
       "      <td>0.014385</td>\n",
       "      <td>0.005216</td>\n",
       "      <td>0.001893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3144168</th>\n",
       "      <td>subj12_series9_145666</td>\n",
       "      <td>0.032445</td>\n",
       "      <td>0.004898</td>\n",
       "      <td>0.006608</td>\n",
       "      <td>0.014589</td>\n",
       "      <td>0.005248</td>\n",
       "      <td>0.001881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3144169</th>\n",
       "      <td>subj12_series9_145667</td>\n",
       "      <td>0.030879</td>\n",
       "      <td>0.005132</td>\n",
       "      <td>0.006641</td>\n",
       "      <td>0.014841</td>\n",
       "      <td>0.005286</td>\n",
       "      <td>0.001873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3144170</th>\n",
       "      <td>subj12_series9_145668</td>\n",
       "      <td>0.029548</td>\n",
       "      <td>0.005384</td>\n",
       "      <td>0.006685</td>\n",
       "      <td>0.015127</td>\n",
       "      <td>0.005325</td>\n",
       "      <td>0.001868</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3144171 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            id  HandStart  FirstDigitTouch  \\\n",
       "0              subj1_series9_0   0.003512         0.004995   \n",
       "1              subj1_series9_1   0.002071         0.012519   \n",
       "2              subj1_series9_2   0.001206         0.016747   \n",
       "3              subj1_series9_3   0.000640         0.024042   \n",
       "4              subj1_series9_4   0.000206         0.064571   \n",
       "...                        ...        ...              ...   \n",
       "3144166  subj12_series9_145664   0.035969         0.004423   \n",
       "3144167  subj12_series9_145665   0.034196         0.004681   \n",
       "3144168  subj12_series9_145666   0.032445         0.004898   \n",
       "3144169  subj12_series9_145667   0.030879         0.005132   \n",
       "3144170  subj12_series9_145668   0.029548         0.005384   \n",
       "\n",
       "         BothStartLoadPhase   LiftOff   Replace  BothReleased  \n",
       "0                  0.006381  0.005685  0.028141      0.016946  \n",
       "1                  0.001428  0.001229  0.026249      0.014232  \n",
       "2                  0.000523  0.000126  0.011038      0.010686  \n",
       "3                  0.000593  0.000115  0.004355      0.022294  \n",
       "4                  0.000786  0.000189  0.002088      0.018881  \n",
       "...                     ...       ...       ...           ...  \n",
       "3144166            0.006587  0.014184  0.005188      0.001910  \n",
       "3144167            0.006590  0.014385  0.005216      0.001893  \n",
       "3144168            0.006608  0.014589  0.005248      0.001881  \n",
       "3144169            0.006641  0.014841  0.005286      0.001873  \n",
       "3144170            0.006685  0.015127  0.005325      0.001868  \n",
       "\n",
       "[3144171 rows x 7 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save final DataFrame to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('prediction'):\n",
    "    os.makedirs('prediction')\n",
    "df.to_csv('prediction/' + 'Deep_isith_FinalResult_padding.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 1.4.0 Py3.7",
   "language": "python",
   "name": "pytorch140_py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
