{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Example\n",
    "\n",
    "### Authors: Brandon G. Jacques and Per B. Sederberg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "**GOAL: Learn to predict what letter is coming next given the context of the entire set of letters that came before it.**\n",
    "\n",
    "In this notebook, we will be covering three timeseries prediction models with identical architectures except for their representation of the past: a Fixed Buffer, an Long Short-Term Memory (LSTM) node, or a Scale-Invariant Temporal History (SITH). The sequence in this case is the speech about the Sith Lord Darth Plagueis the Wise from Star Wars Episode 3. Individual letters will be presented to the model, one at a time as tokenized indices, and the model will have to generate a prediction of what letter should occur next via a log probability. The letter assigned the highest probability is the one that the model will choose to occur next.  \n",
    "\n",
    "This notebook is broken up into a few sections. The first is a data pre-processing section where we clean the entire speach and break tokenize it for input to the model. Then we set up common training and testing code for all the models. Finally, we define a pytorch implementations of all three models, then train each identically, showing graphs of the loss and accuracy as a function of training epoch.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T14:55:42.345036Z",
     "start_time": "2020-04-04T14:55:42.212224Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T14:55:42.779175Z",
     "start_time": "2020-04-04T14:55:42.346176Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cpu\n"
     ]
    }
   ],
   "source": [
    "# Imports, nothing exciting here!\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.nn\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Using:', device)\n",
    "\n",
    "from sith import SITH\n",
    "\n",
    "# You might need to install seaborn if you haven't already\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "sn.set_context(\"poster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize \"Corpus\"\n",
    "\n",
    "Set up the corpus, tokenize the input, and create a list of the target letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T14:55:44.500329Z",
     "start_time": "2020-04-04T14:55:42.780323Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['D', 'I', 'D', ' ', 'Y', 'O', 'U', ' ', 'E', 'V', 'E', 'R', ' ', 'H', 'E', 'A', 'R', ' ', 'T', 'H', 'E', ' ', 'T', 'R', 'A', 'G', 'E', 'D', 'Y', ' ', 'O', 'F', ' ', 'D', 'A', 'R', 'T', 'H', ' ', 'P', 'L', 'A', 'G', 'U', 'E', 'I', 'S', ' ', 'T', 'H', 'E', ' ', 'W', 'I', 'S', 'E', ' ', '.', ' ', 'I', ' ', 'T', 'H', 'O', 'U', 'G', 'H', 'T', ' ', 'N', 'O', 'T', ' ', '.', ' ', 'I', 'T', '<,>', 'S', ' ', 'N', 'O', 'T', ' ', 'A', ' ', 'S', 'T', 'O', 'R', 'Y', ' ', 'T', 'H', 'E', ' ', 'J', 'E', 'D', 'I', ' ', 'W', 'O', 'U', 'L', 'D', ' ', 'T', 'E', 'L', 'L', ' ', 'Y', 'O', 'U', ' ', '.', ' ', 'I', 'T', '<,>', 'S', ' ', 'A', ' ', 'S', 'I', 'T', 'H', ' ', 'L', 'E', 'G', 'E', 'N', 'D', ' ', '.', ' ', 'D', 'A', 'R', 'T', 'H', ' ', 'P', 'L', 'A', 'G', 'U', 'E', 'I', 'S', ' ', 'W', 'A', 'S', ' ', 'A', ' ', 'D', 'A', 'R', 'K', ' ', 'L', 'O', 'R', 'D', ' ', 'O', 'F', ' ', 'T', 'H', 'E', ' ', 'S', 'I', 'T', 'H', ' ', '<,>', ' ', 'S', 'O', ' ', 'P', 'O', 'W', 'E', 'R', 'F', 'U', 'L', ' ', 'A', 'N', 'D', ' ', 'S', 'O', ' ', 'W', 'I', 'S', 'E', ' ', 'H', 'E', ' ', 'C', 'O', 'U', 'L', 'D', ' ', 'U', 'S', 'E', ' ', 'T', 'H', 'E', ' ', 'F', 'O', 'R', 'C', 'E', ' ', 'T', 'O', ' ', 'I', 'N', 'F', 'L', 'U', 'E', 'N', 'C', 'E', ' ', 'T', 'H', 'E', ' ', 'M', 'I', 'D', 'I', 'C', 'H', 'L', 'O', 'R', 'I', 'A', 'N', 'S', ' ', 'T', 'O', ' ', 'C', 'R', 'E', 'A', 'T', 'E', ' ', 'L', 'I', 'F', 'E', ' ', '.', ' ', 'H', 'E', ' ', 'H', 'A', 'D', ' ', 'S', 'U', 'C', 'H', ' ', 'A', ' ', 'K', 'N', 'O', 'W', 'L', 'E', 'D', 'G', 'E', ' ', 'O', 'F', ' ', 'T', 'H', 'E', ' ', 'D', 'A', 'R', 'K', ' ', 'S', 'I', 'D', 'E', ' ', 'T', 'H', 'A', 'T', ' ', 'H', 'E', ' ', 'C', 'O', 'U', 'L', 'D', ' ', 'E', 'V', 'E', 'N', ' ', 'K', 'E', 'E', 'P', ' ', 'T', 'H', 'E', ' ', 'O', 'N', 'E', 'S', ' ', 'H', 'E', ' ', 'C', 'A', 'R', 'E', 'D', ' ', 'A', 'B', 'O', 'U', 'T', ' ', 'F', 'R', 'O', 'M', ' ', 'D', 'Y', 'I', 'N', 'G', ' ', '.', ' ', 'T', 'H', 'E', ' ', 'D', 'A', 'R', 'K', ' ', 'S', 'I', 'D', 'E', ' ', 'O', 'F', ' ', 'T', 'H', 'E', ' ', 'F', 'O', 'R', 'C', 'E', ' ', 'I', 'S', ' ', 'A', ' ', 'P', 'A', 'T', 'H', 'W', 'A', 'Y', ' ', 'T', 'O', ' ', 'M', 'A', 'N', 'Y', ' ', 'A', 'B', 'I', 'L', 'I', 'T', 'I', 'E', 'S', ' ', 'S', 'O', 'M', 'E', ' ', 'C', 'O', 'N', 'S', 'I', 'D', 'E', 'R', ' ', 'T', 'O', ' ', 'B', 'E', ' ', 'U', 'N', 'N', 'A', 'T', 'U', 'R', 'A', 'L', ' ', '.', ' ', 'H', 'E', ' ', 'B', 'E', 'C', 'A', 'M', 'E', ' ', 'S', 'O', ' ', 'P', 'O', 'W', 'E', 'R', 'F', 'U', 'L', ' ', '.', ' ', 'T', 'H', 'E', ' ', 'O', 'N', 'L', 'Y', ' ', 'T', 'H', 'I', 'N', 'G', ' ', 'H', 'E', ' ', 'W', 'A', 'S', ' ', 'A', 'F', 'R', 'A', 'I', 'D', ' ', 'O', 'F', ' ', 'W', 'A', 'S', ' ', 'L', 'O', 'S', 'I', 'N', 'G', ' ', 'H', 'I', 'S', ' ', 'P', 'O', 'W', 'E', 'R', ' ', '<,>', ' ', 'W', 'H', 'I', 'C', 'H', ' ', 'E', 'V', 'E', 'N', 'T', 'U', 'A', 'L', 'L', 'Y', ' ', '<,>', ' ', 'O', 'F', ' ', 'C', 'O', 'U', 'R', 'S', 'E', ' ', '<,>', ' ', 'H', 'E', ' ', 'D', 'I', 'D', ' ', '.', ' ', 'U', 'N', 'F', 'O', 'R', 'T', 'U', 'N', 'A', 'T', 'E', 'L', 'Y', ' ', '<,>', ' ', 'H', 'E', ' ', 'T', 'A', 'U', 'G', 'H', 'T', ' ', 'H', 'I', 'S', ' ', 'A', 'P', 'P', 'R', 'E', 'N', 'T', 'I', 'C', 'E', ' ', 'E', 'V', 'E', 'R', 'Y', 'T', 'H', 'I', 'N', 'G', ' ', 'H', 'E', ' ', 'K', 'N', 'E', 'W', ' ', '<,>', ' ', 'T', 'H', 'E', 'N', ' ', 'H', 'I', 'S', ' ', 'A', 'P', 'P', 'R', 'E', 'N', 'T', 'I', 'C', 'E', ' ', 'K', 'I', 'L', 'L', 'E', 'D', ' ', 'H', 'I', 'M', ' ', 'I', 'N', ' ', 'H', 'I', 'S', ' ', 'S', 'L', 'E', 'E', 'P', ' ', '.', ' ', 'I', 'T', '<,>', 'S', ' ', 'I', 'R', 'O', 'N', 'I', 'C', ' ', 'H', 'E', ' ', 'C', 'O', 'U', 'L', 'D', ' ', 'S', 'A', 'V', 'E', ' ', 'O', 'T', 'H', 'E', 'R', 'S', ' ', 'F', 'R', 'O', 'M', ' ', 'D', 'E', 'A', 'T', 'H', ' ', '<,>', ' ', 'B', 'U', 'T', ' ', 'N', 'O', 'T', ' ', 'H', 'I', 'M', 'S', 'E', 'L', 'F', ' ', '.', ' ']\n"
     ]
    }
   ],
   "source": [
    "# Define what goes at the end of a sentence, and what comes in the middle of \n",
    "# sentences\n",
    "sentence_enders = [\"?\", \".\", \"!\"]\n",
    "separators = [\"\\'\", \",\"]\n",
    "\n",
    "# These are all of the tokens included in the stimuli that our model will be \n",
    "# seeing\n",
    "letters = list(\"abcdefghijklmnopqrstuvwxyz\".upper()) + [\"<,>\", \".\", \" \"]\n",
    "\n",
    "# create letter ids for the tokenizer function later\n",
    "let_to_id = {s:x for x,s in enumerate(letters)}\n",
    "# create a lookup table for model verification later\n",
    "id_to_let = dict([[v,k] for k,v in let_to_id.items()])\n",
    "\n",
    "# Read in the entire text\n",
    "full_text = open(\"plagueis.txt\", \"r\").read()\n",
    "\n",
    "# Make them all captical letters, and remove ellipsis. Add space before each \n",
    "# sentence ender and sentence separator\n",
    "full_text = full_text.upper()\n",
    "full_text = full_text.replace(\"...\", \".\")\n",
    "full_text = full_text.replace(\".\", \" .\")\n",
    "full_text = full_text.replace(\"?\", \" ?\")\n",
    "full_text = full_text.replace(\",\", \" ,\")\n",
    "full_test = full_text.replace(\"\\'\", \" \\'\")\n",
    "\n",
    "# split into individual words\n",
    "split_text = full_text.split()\n",
    "\n",
    "master_list = []\n",
    "id_list = []\n",
    "\n",
    "# Generate list of letter id's to be turned into 1-hot vectors by a \n",
    "# tokenizing function. The letter A has the id 0. The letter Z is 25. etc. \n",
    "for s in split_text:\n",
    "    # loop over each letter in each word\n",
    "    for l in s:\n",
    "        if l in sentence_enders:\n",
    "            master_list.append(\".\")\n",
    "            id_list.append(let_to_id[\".\"])\n",
    "        elif l in separators:\n",
    "            master_list.append(\"<,>\")\n",
    "            id_list.append(let_to_id[\"<,>\"])\n",
    "        else:\n",
    "            master_list.append(l)\n",
    "            id_list.append(let_to_id[l])\n",
    "    # add a space after each word and after each sentence modifier\n",
    "    master_list.append(\" \")\n",
    "    id_list.append(let_to_id[\" \"])\n",
    "\n",
    "# tokenizing funciton, turns each letter into \n",
    "def tokenize(inp_list, num_tokens=10):\n",
    "    output = torch.zeros(len(inp_list), num_tokens, 1).type(torch.DoubleTensor)\n",
    "    for i, inp in enumerate(inp_list):\n",
    "        output[i, inp, 0] = 1.0\n",
    "    return output\n",
    "\n",
    "\n",
    "tokens = torch.DoubleTensor(id_list).view(-1, 1)\n",
    "print(master_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([762, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([761, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inps = tokens[:-1].type(torch.LongTensor)\n",
    "inps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = tokens[1:, 0].type(torch.LongTensor)\n",
    "targets.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing functions Function\n",
    "The goal of the each of the models presented in this notebook are to predict the next letter in the \"Darth Plagius\" speech from Star Wars Episode 2 given the sequence of letters that came before it. The three models will have a very similar structure:\n",
    "\n",
    "    Input -> Embedding -> Temporal Representation -> Hidden -> Output\n",
    "\n",
    "We constructed a single function that could train any of the 3 models we are going to examine in this notebook. Each of the models will take in the entire sequence of the \"Darth Plagius\" speech and calculate the loss based on the predictions that they generate. Each time a model sees the sequence, it will also be subject to a test where we evaluate how many correct answers the model generates. This train_model function will return a dictionary of lists that contains all of the information required to compare the 3 models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, tokens,\n",
    "                optimizer,\n",
    "                loss_func,\n",
    "                epochs=1500):\n",
    "    loss_track = {\"val\":[],\n",
    "                  \"epoch\":[],\n",
    "                  \"acc\":[],\n",
    "                  \"name\":[]}\n",
    "    \n",
    "    \n",
    "    # The inputs to the models are the character id's from let_to_id\n",
    "    # and the targets are just the inps offset by 1. \n",
    "    inps = tokens[:-1].type(torch.cuda.LongTensor)\n",
    "    targets = tokens[1:, 0].type(torch.LongTensor)\n",
    "    \n",
    "    progress_bar = tqdm(range(int(epochs)))\n",
    "    \n",
    "    for e in progress_bar:\n",
    "        # Zero the gradient between each batch\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Present an entire batch to the model\n",
    "        out = model(inps)\n",
    "        print(out.shape)\n",
    "        # Measure loss via CrossEntropyLoss\n",
    "        loss = loss_func(out, targets)\n",
    "        \n",
    "        # Adjust Weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Record loss, epoch number, batch number in epoch, \n",
    "        # last accuracy measure, etc\n",
    "        loss_track['val'].append(loss.mean().detach().cpu().numpy())\n",
    "        loss_track['epoch'].append(e)\n",
    "        loss_track['name'].append(model.__class__.__name__)\n",
    "        \n",
    "        acc = test_model(model, tokens)\n",
    "        loss_track['acc'].append(acc)\n",
    "        \n",
    "        # Update progress_bar\n",
    "        progress_bar.set_description(\"%i: Loss: %0.6f, Acc: %0.4f\" % (e, loss_track['val'][-1], acc))\n",
    "        \n",
    "    return loss_track\n",
    "\n",
    "def test_model(model, tokens):\n",
    "    # Test for accuracy\n",
    "    \n",
    "    inps = tokens[:-1].type(torch.cuda.LongTensor)\n",
    "    targets = tokens[1:,0].type(torch.cuda.LongTensor)\n",
    "    \n",
    "    out = model(inps)\n",
    "\n",
    "    # Accuracy: If the maximum value in the output is at the same\n",
    "    # position as the target id for that input token, then we count it as\n",
    "    # a correct response\n",
    "    correct = (out.argmax(-1) == targets).sum().detach().cpu().numpy()\n",
    "    acc = correct/targets.shape[0]\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buffer\n",
    "A very simple model that tracks the embedding features of each letter for some number of buffer positions into the past. Then passes a flattened, but still correctly ordered, vector of all the features within the buffer through a fully connected linear layer before continuing with the rest of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BufferLetterModel(torch.nn.Module):\n",
    "    def __init__(self, buffer_size, num_tokens, \n",
    "                 embedding_feat, hidden_size,\n",
    "                 buff_cuda=False):\n",
    "        super(BufferLetterModel, self).__init__()\n",
    "        \n",
    "        self._buff_cuda = buff_cuda\n",
    "        \n",
    "        self.letter_embeddings = torch.nn.Embedding(num_tokens, embedding_feat)\n",
    "        self._buffer = torch.zeros((buffer_size, embedding_feat), \n",
    "                                   requires_grad=False).type(torch.DoubleTensor)\n",
    "        if buff_cuda:\n",
    "            self._buffer = self._buffer.type(torch.cuda.DoubleTensor)\n",
    "            \n",
    "        self.linear1 = torch.nn.Linear(embedding_feat*buffer_size, hidden_size)\n",
    "        self.linear2 = torch.nn.Linear(hidden_size, num_tokens)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        if self._buff_cuda:\n",
    "            self._buffer = torch.zeros_like(self._buffer).cuda(device=self._buffer.device)\n",
    "        batch_size = inputs.shape[0]\n",
    "        \n",
    "        output_tensor = torch.zeros(batch_size, \n",
    "                                    self._buffer.shape[0],\n",
    "                                    self._buffer.shape[1]).type(torch.DoubleTensor)\n",
    "        if self._buff_cuda:\n",
    "            output_tensor = output_tensor.cuda(device=self._buffer.device)\n",
    "        embeds = self.letter_embeddings(inputs)\n",
    "        c = 0\n",
    "        for x in embeds.split(1, dim=0):\n",
    "            self._buffer[1:] = self._buffer[:-1].clone()\n",
    "            self._buffer[0] = x.squeeze(0)\n",
    "            output_tensor[c, :, :] = self._buffer.clone()\n",
    "            c += 1\n",
    "        \n",
    "        output_tensor = self.linear1(output_tensor.view(batch_size, -1))\n",
    "        output_tensor = self.linear2(output_tensor)\n",
    "        return output_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  SITH\n",
    "The SITH representation, as detailed elsewhere, is able to track any number of features in an efficently lossy way. That means that this model will take the letter embeddings and track them as $n$ different features within SITH. We then only take a subset of the $\\tau^*$s for each feature and pass them through a linear layer before the rest of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SITHLetterModel(torch.nn.Module):\n",
    "    \"\"\"input shape: (sequence, 1)\"\"\"\n",
    "    def __init__(self, sith_params, num_tokens, hidden_size, sith_cuda=False):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "            sith_params: dictionary\n",
    "                A dictionary contianing all the parameters needed to setup the SITH layer.\n",
    "            \"\"\"\n",
    "        super(SITHLetterModel, self).__init__()\n",
    "        \n",
    "        self.word_embeddings = torch.nn.Embedding(num_tokens, sith_params['in_features'])\n",
    "        \n",
    "        self.sith = SITH(**sith_params)\n",
    "        if sith_cuda:\n",
    "            self.sith.cuda()\n",
    "        \n",
    "        num_taustars = self.sith.tau_star.shape[0]\n",
    "        \n",
    "        self.linear1 = torch.nn.Linear(sith_params['in_features']*num_taustars, hidden_size)\n",
    "        self.linear2 = torch.nn.Linear(hidden_size, num_tokens)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        self.sith.reset()\n",
    "        \n",
    "        batch_size = inputs.shape[0]\n",
    "        \n",
    "        embeds = self.word_embeddings(inputs)\n",
    "        x = self.sith(embeds.squeeze(1)).view(batch_size, -1)\n",
    "        x = self.linear1(x)\n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM\n",
    "\n",
    "Here we simply drop in an LSTM module for the temporal representation. Given that it basically has a hidden layer built into it, we do not add in the extra linear layer like we did for the buffer and SITH models, however, you'll see below that it still ends up with twice as many weights to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = torch.nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = torch.nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        \n",
    "        return tag_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure all models use the same general parameters\n",
    "rep_size = 8\n",
    "hidden_size = 50\n",
    "embed_size = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T14:55:50.854222Z",
     "start_time": "2020-04-04T14:55:50.848368Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BufferLetterModel(\n",
      "  (letter_embeddings): Embedding(29, 15)\n",
      "  (linear1): Linear(in_features=120, out_features=50, bias=True)\n",
      "  (linear2): Linear(in_features=50, out_features=29, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "buff_model = BufferLetterModel(buffer_size=rep_size, \n",
    "                              num_tokens=len(list(let_to_id.keys())),\n",
    "                              embedding_feat=embed_size,\n",
    "                              hidden_size=hidden_size).double()\n",
    "print(buff_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.0000,  1.6006,  2.5620,  4.1008,  6.5638, 10.5062, 16.8166, 26.9170],\n",
      "       dtype=torch.float64)\n",
      "SITHLetterModel(\n",
      "  (word_embeddings): Embedding(29, 15)\n",
      "  (sith): SITH(\n",
      "    out_shape=(1, sequence_len, 8, 15)\n",
      "    (lap): Laplace(15, 1-40 with 150 ntau, k=10, c=0.0251)\n",
      "  )\n",
      "  (linear1): Linear(in_features=120, out_features=50, bias=True)\n",
      "  (linear2): Linear(in_features=50, out_features=29, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "sith_params ={\"in_features\":embed_size,\n",
    "              \"tau_min\":1, \"tau_max\":40, \"k\":10,\n",
    "              \"ntau\":150, \"g\":0.0,\n",
    "              # we want 8 taustars, so we divide the ntau by the\n",
    "              # rep_size to get our desired T_every\n",
    "              \"T_every\":int(150/rep_size)+1, \"alpha\":1.0, 'ttype':torch.DoubleTensor}\n",
    "# This is only for making sure you pick the right parameters\n",
    "sith_model = SITHLetterModel(sith_params=sith_params, \n",
    "                             num_tokens=len(list(let_to_id.keys())), \n",
    "                             hidden_size=hidden_size).double()\n",
    "taustars = sith_model.sith.tau_star\n",
    "print(taustars)\n",
    "print(sith_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMTagger(\n",
      "  (word_embeddings): Embedding(29, 15)\n",
      "  (lstm): LSTM(15, 50)\n",
      "  (hidden2tag): Linear(in_features=50, out_features=29, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "lstm_model = LSTMTagger(embedding_dim=embed_size, \n",
    "                        hidden_dim=hidden_size, \n",
    "                        vocab_size=len(list(let_to_id.keys()))).double()\n",
    "print(lstm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test each model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all models use the same loss function\n",
    "lf = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting Buff\")\n",
    "optim = torch.optim.Adam(buff_model.parameters())\n",
    "perf_buff = train_model(buff_model, tokens, optimizer=optim, loss_func=lf, epochs=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting SITH\")\n",
    "optim = torch.optim.Adam(sith_model.parameters())\n",
    "perf_sith = train_model(sith_model, tokens, optimizer=optim, loss_func=lf, epochs=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting LSTM\")\n",
    "optim = torch.optim.Adam(lstm_model.parameters())\n",
    "perf_lstm = train_model(lstm_model, tokens, optimizer=optim, loss_func=lf, epochs=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T15:01:47.858684Z",
     "start_time": "2020-04-04T15:01:47.831782Z"
    }
   },
   "outputs": [],
   "source": [
    "perf1 = pd.DataFrame(perf_buff)\n",
    "perf2 = pd.DataFrame(perf_sith)\n",
    "perf3 = pd.DataFrame(perf_lstm)\n",
    "results = pd.concat([perf1, perf2, perf3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = []\n",
    "for x in results[\"val\"][:].values:\n",
    "    loss.append(float(x))\n",
    "results.val = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T15:01:48.152509Z",
     "start_time": "2020-04-04T15:01:47.859888Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 8))\n",
    "\n",
    "ax = fig.add_subplot(1,2,1)\n",
    "sn.lineplot(data=results, x='epoch', y='val', hue=\"name\",\n",
    "            ax=ax)\n",
    "ax.set_xlabel(\"Epoch Num\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_title(\"Loss over Batches\")\n",
    "ax = fig.add_subplot(1,2,2)\n",
    "sn.lineplot(data=results, x='epoch', y='acc', hue=\"name\",\n",
    "            ax=ax)\n",
    "ax.set_xlabel(\"Epoch Num\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.set_title(\"Correct Acc over Batches\")\n",
    "\n",
    "# Please note that the line that is in the left graph highly fluctuates\n",
    "# based on the batch within an epoch, so it looks like a filled in area but \n",
    "# is actually just a highly fluctuating line. \n",
    "\n",
    "# it should also be noted that a buffer of size 10 will never reach 100% \n",
    "# accuracy with this model. \n",
    "plt.savefig(\"results_eval\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.bar([\"Buffer\", \"SITH\", \"LSTM\"],\n",
    "        [sum(p.numel() for p in buff_model.parameters()),\n",
    "         sum(p.numel() for p in sith_model.parameters()),\n",
    "         sum(p.numel() for p in lstm_model.parameters())])\n",
    "plt.ylabel(\"Number of Parameters\")\n",
    "plt.savefig(\"weights\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buffer Letter Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(buff_model.letter_embeddings.weight.detach().cpu().numpy().T)\n",
    "df.columns = list(let_to_id.keys())\n",
    "sn.clustermap(figsize=(15,15), data=df.corr(), center=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SITH Letter Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(sith_model.word_embeddings.weight.detach().cpu().numpy().T)\n",
    "df.columns = list(let_to_id.keys())\n",
    "sn.clustermap(figsize=(15,15), data=df.corr(), center=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Letter Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(lstm_model.word_embeddings.weight.detach().cpu().numpy().T)\n",
    "df.columns = list(let_to_id.keys())\n",
    "sn.clustermap(figsize=(15,15), data=df.corr(), center=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
