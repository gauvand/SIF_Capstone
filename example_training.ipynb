{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to set up Rivanna GPU to train Deep_iSith and LSTM models\n",
    "### 1. Use  PyTorch 1.4.0 Py3.7 Kernal/ Container on Rivanna\n",
    "Try to use V100 GPU, since it is much faster than the others\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. pip install the necessary packages and download the [SITH_Layer_master] folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --user mne\n",
    "#!pip install --user seaborn\n",
    "#### pytorch and Cuda should be set up correctly on the Pytorch kernal or pytorch container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Run the code interactively in JupyterLab or by command line\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    3.1 Use the following script: model parameters controlled by config/training_config.ini "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.cuda.DoubleTensor'>\n",
      "<class 'torch.cuda.DoubleTensor'>\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from src.train_util import *\n",
    "from models.Deep_isith_EEG_model import *\n",
    "from models.LSTM_EEG_model import *\n",
    "import pandas as pd\n",
    "# read config file\n",
    "import configparser\n",
    "import argparse\n",
    "\n",
    "# preprocessing\n",
    "import mne\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "import torch.nn\n",
    "import torch.nn.functional as F\n",
    "ttype = torch.cuda.DoubleTensor if torch.cuda.is_available() else torch.DoubleTensor\n",
    "labeltype = torch.cuda.LongTensor if torch.cuda.is_available() else torch.LongTensor\n",
    "print(ttype)\n",
    "from torch.utils.data import Dataset,DataLoader \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# training \n",
    "from torch import nn as nn\n",
    "from math import factorial\n",
    "import random\n",
    "import seaborn as sn\n",
    "import os \n",
    "from os.path import join\n",
    "import glob\n",
    "\n",
    "# validation\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score, matthews_corrcoef,confusion_matrix,plot_roc_curve\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------- sith layer model parameters ------------------#\n",
    "# make sure this in_features matches the number of feutures in the EEG data\n",
    "# Use 3 layers per Per's advice,uses the k-opt code to get optimum  \n",
    "# taumax 50, 200, 800\n",
    "sith_params1 = {\"in_features\":32, \n",
    "                \"tau_min\":1, \"tau_max\":50, \n",
    "                \"k\":23, 'dt':1,\n",
    "                \"ntau\":10, 'g':0.0,  \n",
    "                \"ttype\":ttype, \n",
    "                \"hidden_size\":20, \"act_func\":nn.ReLU()}\n",
    "\n",
    "sith_params2 = {\"in_features\":sith_params1['hidden_size'], \n",
    "                \"tau_min\":1, \"tau_max\":200.0,  \n",
    "                \"k\":12, 'dt':1,\n",
    "                \"ntau\":10, 'g':0.0, \n",
    "                \"ttype\":ttype, \n",
    "                \"hidden_size\":20, \"act_func\":nn.ReLU()}\n",
    "sith_params3 = {\"in_features\":sith_params2['hidden_size'], \n",
    "            \"tau_min\":1, \"tau_max\":800.0,  \n",
    "            \"k\":7, 'dt':1,\n",
    "            \"ntau\":10, 'g':0.0, \n",
    "            \"ttype\":ttype, \n",
    "            \"hidden_size\":20, \"act_func\":nn.ReLU()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to load Subject2 Data.\n",
      "Finished! 7 data are loaded and preprocessed\n",
      "torch.Size([1551, 32, 2000]) torch.Size([1551, 6, 2000])\n",
      "Starting to load Subject2 Data.\n",
      "torch.Size([1, 32, 149945]) torch.Size([1, 6, 149945])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f7ac849803647e48de5f87bab099dbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=80.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a184a63a421e43698b2c6751c8ea5f02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=80.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\"\"\"\n",
    "3.1\n",
    "General-purpose training script for Grasp-and-lift EEG data prediction and classification\n",
    "Currently support:\n",
    "1. A base model for LSTM (Rivanna GPU)\n",
    "2. Deep_isith module\n",
    "**Use Kernal PyTorch 1.4.0 Py3.7**  \n",
    "parts are from **Neural Network Example**(Authors: Brandon G. Jacques and Per B. Sederberg)\n",
    "\n",
    "Yibo Wang\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from src.train_util import *\n",
    "from models.Deep_isith_EEG_model import *\n",
    "from models.LSTM_EEG_model import *\n",
    "import pandas as pd\n",
    "# read config file\n",
    "import configparser\n",
    "import argparse\n",
    "\n",
    "# enable use of command line\n",
    "parser = argparse.ArgumentParser(description='Input config files')\n",
    "parser.add_argument('--config', default = 'config/training_config_Deep_isith.ini', type=str,\n",
    "                    help='an integer for the accumulator')\n",
    "opt, _ = parser.parse_known_args()\n",
    "\n",
    "# parser to read parameters\n",
    "config = configparser.ConfigParser()\n",
    "config.sections()\n",
    "\n",
    "# parameters from config file\n",
    "results = []\n",
    "config.read(opt.config)\n",
    "dir = config['data']['directory']\n",
    "subject_num = int(config['data']['subject #'])\n",
    "kernel_size = int(config['training']['kernel_size'])# sliding window size to use\n",
    "step = int(config['training']['step']) #  --the step between each slice. means overlap between batches is 1- step \n",
    "modelName = config['training']['model']\n",
    "# num of epochs to train\n",
    "nepochs = int(config['training']['nepochs'])\n",
    "loss_func =  torch.nn.CrossEntropyLoss()\n",
    "batch_size = int(config['training']['batch_size']) # batch_size is a hyper parameter to tune \n",
    "lr = float(config['training']['lr'])\n",
    "\n",
    "# load data and do preprocessing\n",
    "train_x_list = []\n",
    "train_y_list = []\n",
    "train_dir = dir + 'train/'\n",
    "val_dir = dir + 'validation/'\n",
    "\n",
    "# load training data\n",
    "print(f\"Starting to load Subject{subject_num} Data.\")\n",
    "for file in os.listdir(train_dir):\n",
    "    sub_idx = file.find('_')\n",
    "    if file[:-4].endswith('_data') & (file[4:sub_idx] == str(subject_num)):\n",
    "        raw = creat_mne_raw_object(train_dir+file,read_events=True)\n",
    "        # filter all channels\n",
    "        input_signal,target_signal = filter_standardization(raw,window_size = 1000,\n",
    "                            l_freq = 0,h_freq = 30)\n",
    "\n",
    "        input_tensor = ttype(input_signal.reshape(1,1,input_signal.shape[0],-1))\n",
    "        target_tensor = labeltype(target_signal.reshape(6,-1)) # should be six channels\n",
    "        input_tensor = input_tensor.squeeze()\n",
    "        # patches data \n",
    "        patches_train = input_tensor.unfold(dimension = 1, size = kernel_size, step = step).permute(1,0,2)\n",
    "        patches_label = target_tensor.unfold(1, kernel_size, step).permute(1,0,2)\n",
    "        #print(patches_train.shape, patches_label.shape)\n",
    "\n",
    "        # append to a list\n",
    "        train_x_list.append(patches_train)\n",
    "        train_y_list.append(patches_label)  \n",
    "        \n",
    "if (not train_x_list) or (not train_y_list):\n",
    "    print(\"No specified data found!\")\n",
    "else:\n",
    "    print(\"Finished! {} data are loaded and preprocessed\".format(len(train_x_list)))\n",
    "\n",
    "# concatenate them\n",
    "train_x_t = torch.cat(train_x_list, dim=0)\n",
    "train_y_t = torch.cat(train_y_list, dim=0)\n",
    "print(train_x_t.shape, train_y_t.shape)\n",
    "\n",
    "val_x_list = []\n",
    "val_y_list = []\n",
    "# load validation data\n",
    "print(f\"Starting to load Subject{subject_num} Data.\")\n",
    "for file in os.listdir(val_dir):\n",
    "    sub_idx = file.find('_')\n",
    "    if file[:-4].endswith('_data') & (file[4:sub_idx] == str(subject_num)):\n",
    "        raw = creat_mne_raw_object(val_dir+file,read_events=True)\n",
    "        # filter all channels\n",
    "        input_signal,target_signal = filter_standardization(raw,window_size = 1000,\n",
    "                            l_freq = 0,h_freq = 30)\n",
    "\n",
    "        input_tensor = ttype(input_signal.reshape(1,1,input_signal.shape[0],-1))\n",
    "        target_tensor = labeltype(target_signal.reshape(6,-1)) # should be six channels\n",
    "        # for batch of 1 only squeeze the first dimension\n",
    "        input_tensor = input_tensor.squeeze(0)\n",
    "        target_tensor = target_tensor.unsqueeze(0)\n",
    "        ###########for validation do not patch data ###########\n",
    "        # patches data \n",
    "        #patches_train = input_tensor.unfold(dimension = 1, size = kernel_size, step = step).permute(1,0,2)\n",
    "        #patches_label = target_tensor.unfold(1, kernel_size, step).permute(1,0,2)\n",
    "        #print(patches_train.shape, patches_label.shape)\n",
    "        val_x_t = input_tensor\n",
    "        val_y_t = target_tensor\n",
    "        #test_y_t = torch.cat(train_y_list, dim=0)\n",
    "        print(val_x_t.shape, val_y_t.shape)\n",
    "        # append to a list\n",
    "        #val_x_list.append(patches_train)\n",
    "        #val_y_list.append(patches_label) \n",
    "        \n",
    "        \n",
    "# start training, iterate thorugh events\n",
    "for i in range(1,7): # There are six events 1 - 6\n",
    "    nClass = i - 1\n",
    "    train_y_t_nClass = train_y_t[:,nClass,:]\n",
    "    val_y_t_nClass = val_y_t[:,nClass,:]\n",
    "    # create dataloader class\n",
    "    train_loader,val_loader = load_data(train_x_t ,train_y_t_nClass,\n",
    "                                             val_x_t ,val_y_t_nClass,\n",
    "                                             batch_size = batch_size)\n",
    "\n",
    "    # match with modelsm currently model name has to be exact\n",
    "    if modelName == 'Deep_isith':\n",
    "        # make a copy of every dict don't want to change them\n",
    "        layer_params = [sith_params1.copy(), sith_params2.copy(),sith_params3.copy()]\n",
    "\n",
    "        #------------------ model configuration ------------------------#\n",
    "        # number of output feature should be 2 since we always train one at a time, so now 1+1\n",
    "        model = DeepSITH_Tracker(out=2,\n",
    "                                    layer_params=layer_params, \n",
    "                                    dropout=0.1).double()\n",
    "    elif modelName == 'LSTM':\n",
    "        hidden_size = 25 # try 256  later\n",
    "        # make sure this in_features matches the number of feutures in the EEG data\n",
    "        model = LSTM_EEG(in_features = 32, hidden_dim = hidden_size, \n",
    "                          out_feuture = 2,num_layers =3, dropout=0.1).double()\n",
    "    else:\n",
    "        print('Model name not recognized!')\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    # map model to GPU\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    #------------------- start training ---------------------------#\n",
    "    perf = []\n",
    "    perf = train_model(model, ttype, train_loader, val_loader,\n",
    "                    optimizer, loss_func, epochs=nepochs)\n",
    "    results.append(perf)\n",
    "\n",
    "\n",
    "    if not os.path.exists('saved_NNs'):\n",
    "        os.makedirs('saved_NNs')\n",
    "    PATH = f'./saved_NNs/{modelName}_Subject{str(subject_num)}_numEvent{nClass}.pth'\n",
    "    torch.save(model.state_dict(), PATH)\n",
    "\n",
    "\n",
    "# save results\n",
    "df = pd.DataFrame()\n",
    "event = ['HandStart','FirstDigitTouch','BothStartLoadPhase',\n",
    "            'LiftOff','Replace','BothReleased']\n",
    "for i in range(len(results)):\n",
    "    perf = results[i]\n",
    "    new_df = pd.DataFrame(perf)\n",
    "    new_df['event'] = event[i]\n",
    "    df = df.append(new_df)\n",
    "if not os.path.exists('csv'):\n",
    "    os.makedirs('csv')\n",
    "csv_name = f'./csv/{modelName}_Subject{str(subject_num)}.csv'\n",
    "df.to_csv(csv_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    3.2  \n",
    "        Use the --config file to specify training parameters.  \n",
    "        Can also be used to specify model used to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# command line\n",
    "# Run in Jupyter notebook/LAb, can be very slow\n",
    "# !python src/train.py --config ./config/training_config_LSTM.ini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.cuda.DoubleTensor'>\n",
      "Starting to load Subject2 Data.\n",
      "/home/yw9et/SIF_Capstone/src/train_util.py:95: RuntimeWarning: 1 channel names are too long, have been truncated to 15 characters:\n",
      "['BothStartLoadPhase']\n",
      "  info = create_info(ch_names,sfreq=500.0, ch_types=ch_type)\n",
      "/home/yw9et/SIF_Capstone/src/train_util.py:95: RuntimeWarning: 1 channel names are too long, have been truncated to 15 characters:\n",
      "['BothStartLoadPhase']\n",
      "  info = create_info(ch_names,sfreq=500.0, ch_types=ch_type)\n",
      "/home/yw9et/SIF_Capstone/src/train_util.py:95: RuntimeWarning: 1 channel names are too long, have been truncated to 15 characters:\n",
      "['BothStartLoadPhase']\n",
      "  info = create_info(ch_names,sfreq=500.0, ch_types=ch_type)\n",
      "/home/yw9et/SIF_Capstone/src/train_util.py:95: RuntimeWarning: 1 channel names are too long, have been truncated to 15 characters:\n",
      "['BothStartLoadPhase']\n",
      "  info = create_info(ch_names,sfreq=500.0, ch_types=ch_type)\n",
      "/home/yw9et/SIF_Capstone/src/train_util.py:95: RuntimeWarning: 1 channel names are too long, have been truncated to 15 characters:\n",
      "['BothStartLoadPhase']\n",
      "  info = create_info(ch_names,sfreq=500.0, ch_types=ch_type)\n",
      "/home/yw9et/SIF_Capstone/src/train_util.py:95: RuntimeWarning: 1 channel names are too long, have been truncated to 15 characters:\n",
      "['BothStartLoadPhase']\n",
      "  info = create_info(ch_names,sfreq=500.0, ch_types=ch_type)\n",
      "/home/yw9et/SIF_Capstone/src/train_util.py:95: RuntimeWarning: 1 channel names are too long, have been truncated to 15 characters:\n",
      "['BothStartLoadPhase']\n",
      "  info = create_info(ch_names,sfreq=500.0, ch_types=ch_type)\n",
      "/home/yw9et/SIF_Capstone/src/train_util.py:95: RuntimeWarning: 1 channel names are too long, have been truncated to 15 characters:\n",
      "['BothStartLoadPhase']\n",
      "  info = create_info(ch_names,sfreq=500.0, ch_types=ch_type)\n",
      "Finished! 8 data are loaded and preprocessed\n",
      "torch.Size([1691, 32, 2000]) torch.Size([1691, 6, 2000])\n",
      "HBox(children=(FloatProgress(value=0.0, max=50.0), HTML(value='')))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/utils/_process_posix.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    160\u001b[0m                 \u001b[0;31m# know whether we've finished (if we matched EOF) or not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0mres_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpect_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatterns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mout_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'replace'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pexpect/spawnbase.py\u001b[0m in \u001b[0;36mexpect_list\u001b[0;34m(self, pattern_list, timeout, searchwindowsize, async_, **kw)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpect_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pexpect/expect.py\u001b[0m in \u001b[0;36mexpect_loop\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    110\u001b[0m                 \u001b[0;31m# Still have time left, so read more data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m                 \u001b[0mincoming\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspawn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_nonblocking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelayafterread\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pexpect/pty_spawn.py\u001b[0m in \u001b[0;36mread_nonblocking\u001b[0;34m(self, size, timeout)\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0;31m# (possibly timeout=None), we call select() with a timeout.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_nonblocking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pexpect/pty_spawn.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(timeout)\u001b[0m\n\u001b[1;32m    448\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mselect_ignore_interrupts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchild_fd\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pexpect/utils.py\u001b[0m in \u001b[0;36mselect_ignore_interrupts\u001b[0;34m(iwtd, owtd, ewtd, timeout)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mselect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miwtd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mowtd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mewtd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-d3861addb065>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'python train_all.py --config ./config/training_config_Deep_isith.ini'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/ipykernel/zmqshell.py\u001b[0m in \u001b[0;36msystem_piped\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    633\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;31m# Ensure new system_piped implementation is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/utils/_process_posix.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0;31m# (the character is known as ETX for 'End of Text', see\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;31m# curses.ascii.ETX).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msendline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m             \u001b[0;31m# Read and print any more output the program might produce on its\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0;31m# way out.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pexpect/pty_spawn.py\u001b[0m in \u001b[0;36msendline\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m    575\u001b[0m         '''\n\u001b[1;32m    576\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_coerce_send_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinesep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_log_control\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pexpect/pty_spawn.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelaybeforesend\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelaybeforesend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_coerce_send_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "!python train_all.py --config ./config/training_config_Deep_isith.ini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Use the train_all.py  \n",
    "### Submit SLURM job to train on Rivanna.  \n",
    "### need to first copy the pytorch-1.4.0-py37.sif container to Desktop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit SLURM job to train on Rivanna\n",
    "# need to first copy the pytorch-1.4.0-py37.sif container to Desktop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#!/bin/bash\n",
    "\n",
    "#SBATCH -o deep_isith.out\n",
    "\n",
    "#SBATCH -e deep_isith.err\n",
    "\n",
    "#SBATCH -p gpu\n",
    "\n",
    "#SBATCH --gres=gpu:v100:1\n",
    "\n",
    "#SBATCH --mem=32000\n",
    "\n",
    "#SBATCH -t 36:00:00\n",
    "\n",
    "#SBATCH -A uva-dsi-msds\n",
    "\n",
    "module load singularity\n",
    "\n",
    "singularity run --nv /home/$USER/pytorch-1.4.0-py37.sif train_all.py --config ./config/training_config_Deep_isith.ini\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 1.4.0 Py3.7",
   "language": "python",
   "name": "pytorch140_py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
